---
layout:     post
title:      kubeadm初始化k8s集群
date:       2019-06-02
author:     cyf
header-img: img/beautiful.jpg
catalog: true
tags:
    - Docker
    - Kubernetes
---
# 一、虚拟机配置
- Virtualbox
- Centos7（CentOS-7-x86_64-Minimal-1810）

配置如下：
```
主机master：2CPU、2G内存、20G磁盘、桥接网络
主机node：2CPU、4G内存、20G磁盘、桥接网络
```
# 二、系统配置
## 1、关闭防火墙
```
[root@master ~]# systemctl stop firewalld.service
[root@master ~]# systemctl disable firewalld.service #禁止开机自启
```
## 2、关闭swap
由于我系统资源不够忽略了swap，安装完kubelet后配置kubelet:
```
[root@master ~]# rpm -ql kubelet 
/etc/kubernetes/manifests
/etc/sysconfig/kubelet
/usr/bin/kubelet
/usr/lib/systemd/system/kubelet.service
[root@master ~]# vi /etc/sysconfig/kubelet
KUBELET_EXTRA_ARGS="--fail-swap-on=false"
[root@master ~]# systemctl enable kubelet #设置开机自启
```
关闭方法：
```
echo "vm.swappiness = 0">> /etc/sysctl.conf 
sysctl -p
```
## 3、配置bridge转发
方法一：配置`sysctl.conf`文件
```
[root@master ~]# vi /etc/sysctl.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
```
遇到的错误：
```
[root@master ~]# sysctl -p
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-ip6tables: No such file or directory
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory
```
解决办法：
```
解决办法:
[root@master ~]# modprobe br_netfilter
[root@master ~]# ls /proc/sys/net/bridge
dr-xr-xr-x 1 root root 0 5月  26 17:35 .
dr-xr-xr-x 1 root root 0 5月  26 17:35 ..
-rw-r--r-- 1 root root 0 5月  27 10:17 bridge-nf-call-arptables
-rw-r--r-- 1 root root 0 5月  26 17:35 bridge-nf-call-ip6tables
-rw-r--r-- 1 root root 0 5月  26 17:35 bridge-nf-call-iptables
-rw-r--r-- 1 root root 0 5月  27 10:17 bridge-nf-filter-pppoe-tagged
-rw-r--r-- 1 root root 0 5月  27 10:17 bridge-nf-filter-vlan-tagged
-rw-r--r-- 1 root root 0 5月  27 10:17 bridge-nf-pass-vlan-input-dev
```
方法二：直接修改`bridge-nf-call-ip6tables`和`bridge-nf-call-iptables`文件
```
[root@master ~]# ls /proc/sys/net/bridge/bridge-nf-call-ip6tables
1
[root@master ~]# ls /proc/sys/net/bridge/bridge-nf-call-iptables
1
```
# 三、下载必要组件
## 1、master节点
kubelet-1.14.2、kubeadm-1.14.2、kubectl-1.14.2、docker-ce
```
[root@master ~]# cd /etc/yum.repos.d
[root@master ~]# wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
[root@master ~]# vi kubernetes.repo
[kubernetes]
name=Kubernetes Repo
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
gpgcheck=0
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
enabled=1
[root@master ~]# yum repolist
[root@master ~]# yum install -y kubelet-1.14.2 kubeadm-1.14.2 kubectl-1.14.2 docker-ce
```
## 2、node节点
 kubelet-1.14.2、kubeadm-1.14.2、docker-ce
```
[root@master ~]# cd /etc/yum.repos.d
[root@master ~]# wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
[root@master ~]# vi kubernetes.repo
[kubernetes]
name=Kubernetes Repo
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
gpgcheck=0
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
enabled=1
[root@master ~]# yum repolist
[root@master ~]# yum install -y kubelet-1.14.2 kubeadm-1.14.2 docker-ce
```
# 四、配置docker（master & node）
启动docker
```
[root@master ~]# systemctl restart docker
[root@master ~]# systemctl enable docker #设置开机自启
```
查看docker驱动
```
[root@master ~]# docker info
...
Cgroup Driver: cgroupfs 
...
```
docker服务的`Cgroup Driver`默认值为`cgroupfs`，当我们用kubeadm初始化的时候会出现如下警告：
```
[WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/
error execution phase preflight: [preflight] Some fatal errors occurred:
```
需要修改`Cgroup Driver`的值为`systemd`
```
[root@master ~]# vi /etc/docker/daemon.json
{
  "exec-opts": ["native.cgroupdriver=systemd"]
}
[root@master ~]# systemctl daemon-reload
[root@master ~]# systemctl restart docker
```
# 五、下载所需镜像
使用如下脚本即可：

k8s-master.sh
```
#!/bin/bash
K8S_VERSION=v1.14.2
ETCD_VERSION=3.3.10
DNS_VERSION=1.3.1
PAUSE_VERSION=3.1
FLANNEL_VERSION=v0.11.0-amd64
# 基本组件
docker pull cuiyf/kube-apiserver:$K8S_VERSION
docker pull cuiyf/kube-controller-manager:$K8S_VERSION
docker pull cuiyf/kube-scheduler:$K8S_VERSION
docker pull cuiyf/kube-proxy:$K8S_VERSION
docker pull cuiyf/etcd:$ETCD_VERSION
docker pull cuiyf/pause:$PAUSE_VERSION
docker pull cuiyf/coredns:$DNS_VERSION
docker pull cuiyf/flannel:$FLANNEL_VERSION
# 修改tag
docker tag cuiyf/kube-apiserver:$K8S_VERSION k8s.gcr.io/kube-apiserver:$K8S_VERSION
docker tag cuiyf/kube-controller-manager:$K8S_VERSION k8s.gcr.io/kube-controller-manager:$K8S_VERSION
docker tag cuiyf/kube-scheduler:$K8S_VERSION k8s.gcr.io/kube-scheduler:$K8S_VERSION
docker tag cuiyf/kube-proxy:$K8S_VERSION k8s.gcr.io/kube-proxy:$K8S_VERSION
docker tag cuiyf/etcd:$ETCD_VERSION k8s.gcr.io/etcd:$ETCD_VERSION
docker tag cuiyf/pause:$PAUSE_VERSION k8s.gcr.io/pause:$PAUSE_VERSION
docker tag cuiyf/coredns:$DNS_VERSION k8s.gcr.io/coredns:$DNS_VERSION
docker tag cuiyf/flannel:$FLANNEL_VERSION
# 删除国内镜像
docker rmi cuiyf/kube-apiserver:$K8S_VERSION
docker rmi cuiyf/kube-controller-manager:$K8S_VERSION
docker rmi cuiyf/kube-scheduler:$K8S_VERSION
docker rmi cuiyf/kube-proxy:$K8S_VERSION
docker rmi cuiyf/etcd:$ETCD_VERSION
docker rmi cuiyf/pause:$PAUSE_VERSION
docker rmi cuiyf/coredns:$DNS_VERSION
docker rmi cuiyf/flannel:$FLANNEL_VERSION
```
k8s-node.sh
```
#!/bin/bash
K8S_VERSION=v1.14.2
DNS_VERSION=1.3.1
PAUSE_VERSION=3.1
FLANNEL_VERSION=v0.11.0-amd64
# 基本组件
docker pull cuiyf/kube-proxy:$K8S_VERSION
docker pull cuiyf/pause:$PAUSE_VERSION
docker pull cuiyf/coredns:$DNS_VERSION
docker pull cuiyf/flannel:$FLANNEL_VERSION
# 修改tag
docker tag cuiyf/kube-proxy:$K8S_VERSION k8s.gcr.io/kube-proxy:$K8S_VERSION
docker tag cuiyf/pause:$PAUSE_VERSION k8s.gcr.io/pause:$PAUSE_VERSION
docker tag cuiyf/coredns:$DNS_VERSION k8s.gcr.io/coredns:$DNS_VERSION
docker tag cuiyf/flannel:$FLANNEL_VERSION
# 删除国内镜像
docker rmi cuiyf/kube-proxy:$K8S_VERSION
docker rmi cuiyf/pause:$PAUSE_VERSION
docker rmi cuiyf/coredns:$DNS_VERSION
docker rmi cuiyf/flannel:$FLANNEL_VERSION
```
# 六、kubeadm初始化集群
## 1、master节点初始化：kubeadm init
```
[root@master ~]# kubeadm init --kubernetes-version=v1.14.2 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 --ignore-preflight-errors=Swap
[init] Using Kubernetes version: v1.14.2
[preflight] Running pre-flight checks
	[WARNING Swap]: running with swap on is not supported. Please disable swap
	[WARNING SystemVerification]: this Docker version is not on the list of validated versions: 19.03.1. Latest validated version: 18.09
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Activating the kubelet service
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [master localhost] and IPs [192.168.10.48 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [master localhost] and IPs [192.168.10.48 127.0.0.1 ::1]
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.10.48]
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 18.006953 seconds
[upload-config] storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.14" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --experimental-upload-certs
[mark-control-plane] Marking the node master as control-plane by adding the label "node-role.kubernetes.io/master=''"
[mark-control-plane] Marking the node master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: e7c4y8.49l41qmlyxqpzebm
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] creating the "cluster-info" ConfigMap in the "kube-public" namespace
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.10.48:6443 --token e7c4y8.49l41qmlyxqpzebm \
    --discovery-token-ca-cert-hash sha256:78b86fcae0bb7101365495507c38f260b2729f012b17e6f577272164bc3781bf  
    
[root@master ~]# mkdir -p $HOME/.kube
[root@master ~]# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
[root@master ~]# chown $(id -u):$(id -g) $HOME/.kube/config
[root@master ~]# ss -tln | grep 6443 #查看6443端口是否开启
LISTEN     0      128         :::6443                    :::* 
```
如果忘记加入集群命令，使用如下命令：
```
[root@master ~]# kubeadm token create --print-join-command
kubeadm join 192.168.10.130:6443 --token zac70e.6aqzqc3g4zytxzmf --discovery-token-ca-cert-hash sha256:78b86fcae0bb7101365495507c38f260b2729f012b17e6f577272164bc3781bf 
```
## 2、node节点加入集群
由于我没有关闭swap，需要使用`--ignore-preflight-errors`忽略swap警告（注意Swap首字母大写）
```
[root@node ~]# kubeadm join 192.168.10.130:6443 --token zac70e.6aqzqc3g4zytxzmf --discovery-token-ca-cert-hash sha256:78b86fcae0bb7101365495507c38f260b2729f012b17e6f577272164bc3781bf --ignore-preflight-errors=Swap
```
## 3、查看节点状态
```
[root@master ~]#  kubectl get nodes
NAME     STATUS     ROLES    AGE   VERSION
master   NotReady   master   81s   v1.14.2
node1    NotReady   <none>   36s   v1.14.2
```
此时此时节点状态为`NotReady`，需要配置`flannel`组件
```
[root@master ~]# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
podsecuritypolicy.extensions/psp.flannel.unprivileged created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.extensions/kube-flannel-ds-amd64 created
daemonset.extensions/kube-flannel-ds-arm64 created
daemonset.extensions/kube-flannel-ds-arm created
daemonset.extensions/kube-flannel-ds-ppc64le created
daemonset.extensions/kube-flannel-ds-s390x created
```
再次查看节点状态，发现是`Ready`状态即可
```
[root@master ~]#  kubectl get nodes
NAME     STATUS   ROLES    AGE   VERSION
master   Ready    master   92s   v1.14.2
node1    Ready    <none>   47s   v1.14.2
```
此时`ROLES`为`<none>`状态，修改node节点角色
```
[root@master ~]# kubectl label node node1 node-role.kubernetes.io/worker=worker
node/k8s-node labeled
[root@master ~]# kubectl get nodes
NAME         STATUS   ROLES    AGE   VERSION
NAME     STATUS   ROLES    AGE   VERSION
master   Ready    master   92s   v1.14.2
node1    Ready    worker   47s   v1.14.2
```
查看所有组件启动状态，均为`running`状态即可
```
[root@master ~]#  kubectl get pods -A -o wide
NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE     IP               NODE     NOMINATED NODE   READINESS GATES
kube-system   coredns-fb8b8dccf-b9cwb          1/1     Running   0          9m23s   10.244.1.5       node1    <none>           <none>
kube-system   coredns-fb8b8dccf-qcdv2          1/1     Running   0          9m23s   10.244.1.4       node1    <none>           <none>
kube-system   etcd-master                      1/1     Running   0          8m28s   192.168.10.48    master   <none>           <none>
kube-system   kube-apiserver-master            1/1     Running   0          8m16s   192.168.10.48    master   <none>           <none>
kube-system   kube-controller-manager-master   1/1     Running   0          8m22s   192.168.10.48    master   <none>           <none>
kube-system   kube-flannel-ds-amd64-4kn7q      1/1     Running   0          8m24s   192.168.10.48    master   <none>           <none>
kube-system   kube-flannel-ds-amd64-xpd8f      1/1     Running   0          8m24s   192.168.10.148   node1    <none>           <none>
kube-system   kube-proxy-htdpc                 1/1     Running   0          8m57s   192.168.10.148   node1    <none>           <none>
kube-system   kube-proxy-q49ff                 1/1     Running   0          9m23s   192.168.10.48    master   <none>           <none>
kube-system   kube-scheduler-master            1/1     Running   0          8m21s   192.168.10.48    master   <none>           <none>
```
至此kubeadm初始化完成
# 参考资料
[virtualbox安装centos7教程示例](https://www.jianshu.com/p/18207167b1e7)

[B站----2018黑马docker容器技术+k8s集群技术](https://www.bilibili.com/video/av35847195/?p=13)